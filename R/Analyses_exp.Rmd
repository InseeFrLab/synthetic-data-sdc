---
title: "Analyses des simulations"
author: "Julien Helfenstein"
date: "`r Sys.Date()`"
output: html_document
---

```{r packages}
source("Analyses.R")
```

```{r données}
data <- res_simul 
```

## Vue d'ensemble

```{r}
table_cat
table_mean_num
```

## Correlations

```{r correlations}
cor_comp
somme_cor_mat
```

liste_mat est une liste de 6 matrices. Ces matrices sont les matrices de corrélations moyenne des simulations pour chaque modèles moins la matrice de corrélation du jeu de données original.

somme_cor_mat est la somme des éléments de chaque matrices. Plus la valeur est proche de 0 plus les synthétisations produisent de bonnes corrélations.

bag et cart sont ici les meilleurs modèles suivis par parametric, ctree et rf. Sample est loin derrière.

## MAE
```{r mae}
  table_mae
```
On remarque que bag et rf ont des MAE très élevé allant jusqu'à être 10 fois supérieurs aux autres modèles. Sample, cart, ctree et parametric sont assez rapprochés sans qu'il n'y ait une réelle domination d'un modèle. 

## MSE
```{r mse}
  table_mse
```
On ne voit pas de réelle domination d'un modèle.


## Analyse de l'IMC

On va vérifier deux choses :

-   Est-ce que dans les jeux de données synthétiques la variable bmi est bien synthétisé, i.e est-ce qu'elle respecte bmi = 10000 \* (weight / height \^ 2) ?

-   Est-ce que les valeurs de la variable bmi synthétisée sont proches de celles de la variable bmi originale ?

```{r bmi comparaison}
bmi_comp(data)
```

1)  On constate une assez bonne synthétisation via cart, ctree, parametric et bag, une moins bonne synthétisation via rf et une vraiment moins bonne synthétisation via sample. On n'a cependant pas une valeur de 0 ce qui signifie que le modèle s'approche de la relation mais ne la reproduit pas totalement.

2)  En ce qui concerne la comparaison au jeu de données original, sample, cart, ctree et bagging s'en sortent le mieux, suivis par rf puis parametric.

```{r}
  reg_coeff[2, ]
```
reg_coeff est un vecteur comportant la moyenne des coefficients directeurs pour chaque modèles.

On voit que bag, cart et ctree sont très proches, suivis par parametric (x5), rf(x11). Sample est à la ramasse

```{r graphiques bmi}
var_test_org <- 10000 * (data$original[, "weight"] / (data$original[, "height"])^2)
reglinorg <- lm(bmi ~ var_test_org, data = data$original)

# Définir les coefficients de régression pour chaque modèle
intercepts <- reg_coeff["intercept", ]
slopes <- reg_coeff["coeff", ]

# Créer une séquence de points pour l'axe x
x_vals <- seq(0, 100, length.out = 100)

# Initialiser le graphique
plot(x_vals, x_vals, type = "n", xlab = "x", ylab = "y", xlim = c(0, 100), ylim = c(-5, 100))
colors <- rainbow(length(mes_modeles))
for (i in 1:length(mes_modeles)) {
  y_vals <- intercepts[i] + slopes[i] * x_vals
  lines(x_vals, y_vals, col = colors[i], lwd = 2)
}
y_org <- reglinorg$coefficients[1] + reglinorg$coefficients[2] * x_vals
lines(x_vals, y_org, col = "black", lwd = 2)
legend("bottomright", legend = mes_modeles, col = colors, lwd = 0.5, title = "Modèles")
title("Droites de Régression")
```

## Mesures d'utilités
```{r utilite}
utility_measures_all_meth
```


```{r graphiques utilite}
utility_measures_all_meth <- utility_measures_all_meth %>%
  group_by(method) %>% 
  mutate(i = 1:500) %>% 
  mutate(across(pMSE:U, cummean, .names = "{.col}_cummean"))

utility_measures_all_meth %>% 
  ggplot() +
  geom_line(aes(x = i, y = pMSE_cummean, col = method))


utility_graph <- map(
  c("pMSE","SPECKS","PO50","U"),
  \(meth){
    utility_measures_all_meth %>% 
      ggplot( aes(x=method, y=.data[[meth]], fill=method)) +
      geom_violin(width=1.4) +
      geom_boxplot(width=0.1, color="grey", alpha=0.2) +
      scale_fill_viridis(discrete = TRUE) +
      coord_flip() +
      theme_ipsum() +
      theme(
        legend.position="none",
        plot.title = element_text(size=11)
      ) +
      ggtitle(paste0("distribution des ", meth)) +
      ylab(meth) + xlab("méthode")
  }
)
names(utility_graph) <- c("pMSE","SPECKS","PO50","U")
utility_graph$pMSE
utility_graph$SPECKS
utility_graph$PO50
utility_graph$U

utility_measures_summary <- utility_measures_all_meth %>% 
  group_by(method) %>% 
  summarise(
    across(
      pMSE:U, 
      list(mean = mean, min = min, max = max, 
           pc025 = ~quantile(., probs = 0.025),
           pc975 = ~quantile(., probs = 0.975),
           sd = sd
      ),
      .names = "{.col}_{.fn}"
    )) %>%
  tidyr::pivot_longer(-1, names_to = "indicateur", values_to = "val") %>% 
  tidyr::separate(indicateur, into = c("utility","indicateur"))

utility_measures_summary %>% 
  filter(indicateur == "mean") %>%
  ggplot() +
  geom_bar(aes(x = method, y = val, fill = utility), stat = "identity")+
  coord_flip() +
  facet_wrap(~utility, scales = "free") +
  theme_ipsum()
```
1) Sur le premier graphique, on remarque que les moyennes cumulées se stabilisent très rapidement. On remarque de plus que cart est le meilleur modèle en terme de pMSE. Il est suivi par rf, bag et ctree. Parametric arrive ensuite avec un pMSE environ 2 fois plus élevé. Enfin, sample a un pMSE d'environ de l'ordre de 7 fois supérieur.

2) On constate que pour la plupart des modèles, l'étendue du pMSE est relativement contenue à l'exception de sample. Cart est ici le meilleur et sample est le moins bon et de loin.

3) Concernant le SPECKS, on obtient les mêmes conclusions que pour le pMSE. On rappelle que le SPECKS est défini par : $$SPECKS =  $$
4) De même pour le PO50. On rappelle qu'il est défini par : $$PO50 =  $$

## Recherche des individus répliqués
```{r repliques}
nb_repliques_all_meth %>% 
  group_by(method) %>% 
  summarise(across(n_replicats, list(mean=mean, min=min, max=max, sd=sd)))
```